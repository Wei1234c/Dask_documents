{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Locality\n",
    "https://distributed.readthedocs.io/en/latest/locality.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Control\n",
    "Complex algorithms may require more user control.\n",
    "\n",
    "For example the existence of specialized hardware such as GPUs or database connections may restrict the set of valid workers for a particular task.\n",
    "\n",
    "In these cases use the workers= keyword argument to the submit, map, or scatter functions, providing a hostname, IP address, or alias as follows:\n",
    "```python\n",
    "future = client.submit(func, *args, workers=['Alice'])\n",
    "Alice: [0, 1, 4, 5, 8, 9, new_result]\n",
    "Bob: [2, 3, 6, 7]\n",
    "``` \n",
    "Required data will always be moved to these workers, even if the volume of that data is significant. If this restriction is only a preference and not a strict requirement, then add the allow_other_workers keyword argument to signal that in extreme cases such as when no valid worker is present, another may be used.\n",
    "```python\n",
    "future = client.submit(func, *args, workers=['Alice'],\n",
    "                       allow_other_workers=True)\n",
    "```\n",
    "Additionally the scatter function supports a broadcast= keyword argument to enforce that the all data is sent to all workers rather than round-robined. If new workers arrive they will not automatically receive this data.\n",
    "```python\n",
    "futures = client.scatter([1, 2, 3], broadcast=True)  # send data to all workers\n",
    "Alice: [1, 2, 3]\n",
    "Bob: [1, 2, 3]\n",
    "```\n",
    "Valid arguments for workers= include the following:\n",
    "\n",
    "A single IP addresses, IP/Port pair, or hostname like the following:\n",
    "```\n",
    "192.168.1.100, 192.168.1.100:8989, alice, alice:8989\n",
    "```\n",
    "A list or set of the above:\n",
    "```python\n",
    "['alice'], ['192.168.1.100', '192.168.1.101:9999']\n",
    "```\n",
    "If only a hostname or IP is given then any worker on that machine will be considered valid. Additionally, you can provide aliases to workers upon creation.:\n",
    "```\n",
    "$ dask-worker scheduler_address:8786 --name worker_1\n",
    "```\n",
    "And then use this name when specifying workers instead.\n",
    "```python\n",
    "client.map(func, sequence, workers='worker_1')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify workers with Compute/Persist\n",
    "The workers= keyword in scatter, submit, and map is fairly straightforward, taking either a worker hostname, host:port pair or a sequence of those as valid inputs:\n",
    "```python\n",
    "client.submit(f, x, workers='127.0.0.1')\n",
    "client.submit(f, x, workers='127.0.0.1:55852')\n",
    "client.submit(f, x, workers=['192.168.1.101', '192.168.1.100'])\n",
    "```\n",
    "For more complex computations, such as occur with dask collections like dask.dataframe or dask.delayed, we sometimes want to specify that certain parts of the computation run on certain workers while other parts run on other workers.\n",
    "```python\n",
    "x = delayed(f)(1)\n",
    "y = delayed(f)(2)\n",
    "z = delayed(g)(x, y) \n",
    "\n",
    "future = client.compute(z, workers={z: '127.0.0.1',\n",
    "                                    x: '192.168.0.1'})\n",
    "```\n",
    "Here the values of the dictionary are of the same form as before, a host, a host:port pair, or a list of these. The keys in this case are either dask collections or tuples of dask collections. All of the final keys of these collections will run on the specified machines; dependencies can run anywhere unless they are also listed in workers=. We explore this through a set of examples:\n",
    "```python\n",
    "The computation z = f(x, y) runs on the host 127.0.0.1. The other two computations for x and y can run anywhere.\n",
    "```\n",
    "future = client.compute(z, workers={z: '127.0.0.1'})\n",
    "The computations for both z and x must run on 127.0.0.1\n",
    "```python\n",
    "future = client.compute(z, workers={z: '127.0.0.1',\n",
    "                                    x: '127.0.0.1'})\n",
    "```\n",
    "Use a tuple to group collections. This is shorthand for the above.\n",
    "```python\n",
    "future = client.compute(z, workers={(x, y): '127.0.0.1'})\n",
    "```\n",
    "Recall that all options for workers= in scatter/submit/map hold here as well.\n",
    "```python\n",
    "future = client.compute(z, workers={(x, y): ['192.168.1.100', '192.168.1.101:9999']})\n",
    "```\n",
    "Set allow_other_workers=True to make these loose restrictions rather than hard requirements.\n",
    "```python\n",
    "future = client.compute(z, workers={(x, y): '127.0.0.1'},\n",
    "                        allow_other_workers=True)\n",
    "```\n",
    "Provide a collection to allow_other_workers=[...] to say that the keys for only some of the collections are loose. In the case below z must run on 127.0.0.1 while x should run on 127.0.0.1 but can run elsewhere if necessary:\n",
    "```python\n",
    "future = client.compute(z, workers={(x, y): '127.0.0.1'},\n",
    "                        allow_other_workers=[x])\n",
    "```\n",
    "This works fine with persist and with any dask collection (any object with a .__dask_graph__() method):\n",
    "```python\n",
    "df = dd.read_csv('s3://...')\n",
    "df = client.persist(df, workers={df: ...})\n",
    "```\n",
    "See the efficiency page to learn about best practices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
