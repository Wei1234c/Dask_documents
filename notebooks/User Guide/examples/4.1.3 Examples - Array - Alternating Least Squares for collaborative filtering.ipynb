{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples - Array - Alternating Least Squares for collaborative filtering\n",
    "https://gist.github.com/mrocklin/017f78ce52d265b6d72828fb29e5619c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Decompositions with NumPy and Dask.array\n",
    "We implement a very simple algorithm to compute the PARAFAC/CANDECOMP tensor decomposition using alternating least squares.\n",
    "\n",
    "We follow section 4.1 from this paper: https://www.cs.cmu.edu/~pmuthuku/mlsp_page/lectures/Parafac.pdf\n",
    "\n",
    "PARAFAC/CANDECOMP is an array decomposition that generalizes SVD/PCA to higher dimensions. It is commonly used when trying to find low-rank structure in observational data in multiple dimensions.\n",
    "\n",
    "<img src='https://camo.githubusercontent.com/a6e062883b83adb3b65b5a9e167a3a6f5e5f9a19/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f352f35322f436f6c6c61626f7261746976655f66696c746572696e672e676966'>\n",
    "\n",
    "In a typical two dimensional setting we might call this collaborative filtering, and is an approach used by companies like Amazon and Netflix to help recommend products to users based on the habits of similar users. However nothing about this problem is specifically restricted to two dimensions. We might consider the following applications:\n",
    "\n",
    "- What customers bought what products at what times of day or year\n",
    "- Which computers talked to which other computers over which ports under which user\n",
    "- Outcomes of treatments affecting patients with which dieseases\n",
    "\n",
    "When we look around for algorithms to generalize SVD/PCA to multiple dimensions we come across a family of algorithms called **tensor decompositions**. This notebook implements the simplest such algorithm in a common and simple way using NumPy and then scales it out naively using Dask arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "import operator\n",
    "import dask\n",
    "\n",
    "\n",
    "def outer_product(*Xs):\n",
    "    \"\"\" Outer/tensor product of multiple 2d arrays\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *Xs: arrays of size (k, n)\n",
    "        All inputs must have the same first dimension but may have varying\n",
    "        second dimensions\n",
    "    \"\"\"\n",
    "    n = len(Xs)\n",
    "    indexes = [(slice(None, None),) +\n",
    "               (None,) * i +\n",
    "               (slice(None, None),) +\n",
    "               (None,) * (n - i - 1) for i in range(len(Xs))]\n",
    "    Ys = [X[ind] for X, ind in zip(Xs, indexes)]\n",
    "    Ys = sorted(Ys, key=lambda y: y.nbytes)  # smaller outer products first\n",
    "    return reduce(operator.mul, Ys)\n",
    "\n",
    "\n",
    "def parafac_als(X, n_factors, n_iter=100):\n",
    "    \"\"\" Parafac tensor decomposition\n",
    "\n",
    "    This implements the basic algorithm in section 4.1 of this paper\n",
    "\n",
    "        https://www.cs.cmu.edu/~pmuthuku/mlsp_page/lectures/Parafac.pdf\n",
    "    \"\"\"\n",
    "    # Randomly initialize factors\n",
    "    factors = [np.random.random((n_factors, X.shape[i])) for i in range(0, X.ndim)]\n",
    "    \n",
    "    # Solve\n",
    "    for itr in range(n_iter):\n",
    "        for i in range(X.ndim):\n",
    "            not_i = tuple(j for j in range(X.ndim) if j != i)\n",
    "            Xp = X.transpose((i,) + not_i)\n",
    "            Xp = Xp.reshape((Xp.shape[0], np.prod(Xp.shape[1:])))\n",
    "            Z = outer_product(*[factors[j] for j in not_i])\n",
    "            Z = Z.reshape((Z.shape[0], np.prod(Z.shape[1:])))\n",
    "\n",
    "            factor, residuals, rank, s = np.linalg.lstsq(Z.T, Xp.T)\n",
    "            factors[i] = factor\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1, 1, 0, 1, 1],\n",
       "         [1, 1, 1, 2, 1]],\n",
       "\n",
       "        [[1, 1, 0, 1, 1],\n",
       "         [1, 1, 0, 1, 1]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 1, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[1, 1, 0, 1, 1],\n",
       "         [1, 1, 1, 2, 1]],\n",
       "\n",
       "        [[1, 1, 0, 1, 1],\n",
       "         [1, 1, 0, 1, 1]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[1, 1, 0, 1, 1],\n",
       "         [1, 1, 0, 1, 1]],\n",
       "\n",
       "        [[1, 1, 0, 1, 1],\n",
       "         [1, 1, 0, 1, 1]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0]]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_factors = [[[1, 1, 1, 0, 0, 0],\n",
    "                 [1, 0, 1, 0, 1, 0]],\n",
    "                [[1, 0],\n",
    "                 [1, 1]],\n",
    "                [[0, 1],\n",
    "                 [1, 1]],\n",
    "                [[0, 0, 1, 1, 0],\n",
    "                 [1, 1, 0, 1, 1]]]\n",
    "true_factors = [np.array(x).reshape((2, len(x[0]))) for x in true_factors]\n",
    "\n",
    "X = outer_product(*true_factors).sum(axis=0)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 9.94701717e-01,  9.94701717e-01,  1.90151086e-04,\n",
       "           9.94891868e-01,  9.94701717e-01],\n",
       "         [ 9.98483286e-01,  9.98483286e-01,  1.01228942e+00,\n",
       "           2.01077271e+00,  9.98483286e-01]],\n",
       "\n",
       "        [[ 1.00206577e+00,  1.00206577e+00, -3.23840338e-03,\n",
       "           9.98827364e-01,  1.00206577e+00],\n",
       "         [ 9.94457318e-01,  9.94457318e-01,  1.59216427e-02,\n",
       "           1.01037896e+00,  9.94457318e-01]]],\n",
       "\n",
       "\n",
       "       [[[-5.72293108e-03, -5.72293108e-03,  4.69442356e-03,\n",
       "          -1.02850751e-03, -5.72293108e-03],\n",
       "         [ 1.27144533e-02,  1.27144533e-02,  9.29383638e-01,\n",
       "           9.42098092e-01,  1.27144533e-02]],\n",
       "\n",
       "        [[-4.38539968e-03, -4.38539968e-03, -4.97641313e-03,\n",
       "          -9.36181281e-03, -4.38539968e-03],\n",
       "         [-3.12551541e-02, -3.12551541e-02,  1.36705758e-01,\n",
       "           1.05450603e-01, -3.12551541e-02]]],\n",
       "\n",
       "\n",
       "       [[[ 9.94701717e-01,  9.94701717e-01,  1.90151086e-04,\n",
       "           9.94891868e-01,  9.94701717e-01],\n",
       "         [ 9.98483286e-01,  9.98483286e-01,  1.01228942e+00,\n",
       "           2.01077271e+00,  9.98483286e-01]],\n",
       "\n",
       "        [[ 1.00206577e+00,  1.00206577e+00, -3.23840338e-03,\n",
       "           9.98827364e-01,  1.00206577e+00],\n",
       "         [ 9.94457318e-01,  9.94457318e-01,  1.59216427e-02,\n",
       "           1.01037896e+00,  9.94457318e-01]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 1.00042465e+00,  1.00042465e+00, -4.50427248e-03,\n",
       "           9.95920375e-01,  1.00042465e+00],\n",
       "         [ 9.85768833e-01,  9.85768833e-01,  8.29057820e-02,\n",
       "           1.06867461e+00,  9.85768833e-01]],\n",
       "\n",
       "        [[ 1.00645117e+00,  1.00645117e+00,  1.73800975e-03,\n",
       "           1.00818918e+00,  1.00645117e+00],\n",
       "         [ 1.02571247e+00,  1.02571247e+00, -1.20784115e-01,\n",
       "           9.04928357e-01,  1.02571247e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors = parafac_als(X, 3, 10)\n",
    "\n",
    "computed = outer_product(*factors).sum(axis=0)\n",
    "computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0., -0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[-0., -0.,  0., -0., -0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[-0., -0., -0., -0., -0.],\n",
       "         [-0., -0.,  0.,  0., -0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0., -0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0., -0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0., -0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed.round(0) - X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale with Dask.array\n",
    "The ALS algorithm above is largely reshapings, scalar multiplication, matrix multiplication, and least squares computations (A QR decomposition followed by a triangular solve) all of which Dask.array can do easily. Here we adapt our code from before to work on dask.arrays. We include the original unchanged lines to show how easy it is to adapt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "def parafac_als(X, n_factors, n_iter=100):\n",
    "    \"\"\" Parafac tensor decomposition\n",
    "\n",
    "    This implements the basic algorithm in section 4.1 of this paper\n",
    "\n",
    "        https://www.cs.cmu.edu/~pmuthuku/mlsp_page/lectures/Parafac.pdf\n",
    "    \"\"\"\n",
    "    # Randomly initialize factors\n",
    "    # factors = [np.random.random((n_factors, X.shape[i])) for i in range(0, X.ndim)]\n",
    "    factors = [da.random.random((n_factors, X.shape[i]), \n",
    "                                chunks=(None, X.chunks[i])) \n",
    "               for i in range(0, X.ndim)]\n",
    "    \n",
    "    # Solve\n",
    "    for itr in range(n_iter):\n",
    "        for i in range(X.ndim):\n",
    "            not_i = tuple(j for j in range(X.ndim) if j != i)\n",
    "            Xp = X.transpose((i,) + not_i)\n",
    "            Xp = Xp.reshape((Xp.shape[0], np.prod(Xp.shape[1:])))\n",
    "            Z = outer_product(*[factors[j] for j in not_i])\n",
    "            Z = Z.reshape((Z.shape[0], np.prod(Z.shape[1:])))\n",
    "\n",
    "            # factor, residuals, rank, s = np.linalg.lstsq(Z.T, Xp.T)\n",
    "            factor, residuals, rank, s = da.linalg.lstsq(Z.T, Xp.T)\n",
    "            factors[i] = factor\n",
    "            \n",
    "    return factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize result\n",
    "To show the complexity of the algorithm we visualize a single round of chunked ALS as a task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/anaconda3/lib/python3.6/site-packages/dask/array/linalg.py:791: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  np.array([0, 1], dtype=b.dtype))\n"
     ]
    }
   ],
   "source": [
    "dX = da.from_array(X, chunks=((2, 2, 2, 2)))\n",
    "factors = parafac_als(dX, 2, 1)\n",
    "\n",
    "# dask.visualize(*factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify accuracy\n",
    "We can compare against the numpy version to verify that we continue to get the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/anaconda3/lib/python3.6/site-packages/dask/array/linalg.py:791: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  np.array([0, 1], dtype=b.dtype))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.97573206,  0.97573206,  0.03464583,  1.01037789,\n",
       "           0.97573206],\n",
       "         [ 0.99723333,  0.99723333,  1.01331671,  2.01055004,\n",
       "           0.99723333]],\n",
       "\n",
       "        [[ 1.01702812,  1.01702812, -0.0470655 ,  0.96996262,\n",
       "           1.01702812],\n",
       "         [ 0.98032032,  0.98032032,  0.03852199,  1.01884232,\n",
       "           0.98032032]]],\n",
       "\n",
       "\n",
       "       [[[-0.03196896, -0.03196896,  0.07780312,  0.04583417,\n",
       "          -0.03196896],\n",
       "         [ 0.02343247,  0.02343247,  0.93261202,  0.95604449,\n",
       "           0.02343247]],\n",
       "\n",
       "        [[-0.03809417, -0.03809417,  0.00853454, -0.02955963,\n",
       "          -0.03809417],\n",
       "         [-0.03190624, -0.03190624,  0.0814083 ,  0.04950205,\n",
       "          -0.03190624]]],\n",
       "\n",
       "\n",
       "       [[[ 0.97573206,  0.97573206,  0.03464583,  1.01037789,\n",
       "           0.97573206],\n",
       "         [ 0.99723333,  0.99723333,  1.01331671,  2.01055004,\n",
       "           0.99723333]],\n",
       "\n",
       "        [[ 1.01702812,  1.01702812, -0.0470655 ,  0.96996262,\n",
       "           1.01702812],\n",
       "         [ 0.98032032,  0.98032032,  0.03852199,  1.01884232,\n",
       "           0.98032032]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 1.00770102,  1.00770102, -0.04315729,  0.96454372,\n",
       "           1.00770102],\n",
       "         [ 0.97380086,  0.97380086,  0.0807047 ,  1.05450555,\n",
       "           0.97380086]],\n",
       "\n",
       "        [[ 1.05512229,  1.05512229, -0.05560004,  0.99952225,\n",
       "           1.05512229],\n",
       "         [ 1.01222657,  1.01222657, -0.0428863 ,  0.96934026,\n",
       "           1.01222657]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import dask\n",
    "\n",
    "factors = parafac_als(dX, 2, 10)\n",
    "\n",
    "computed = outer_product(*factors).sum(axis=0).compute()\n",
    "computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0., -0.,  0.,  0., -0.],\n",
       "         [-0., -0.,  0.,  0., -0.]],\n",
       "\n",
       "        [[ 0.,  0., -0., -0.,  0.],\n",
       "         [-0., -0.,  0.,  0., -0.]]],\n",
       "\n",
       "\n",
       "       [[[-0., -0.,  0.,  0., -0.],\n",
       "         [ 0.,  0., -0., -0.,  0.]],\n",
       "\n",
       "        [[-0., -0.,  0., -0., -0.],\n",
       "         [-0., -0.,  0.,  0., -0.]]],\n",
       "\n",
       "\n",
       "       [[[-0., -0.,  0.,  0., -0.],\n",
       "         [-0., -0.,  0.,  0., -0.]],\n",
       "\n",
       "        [[ 0.,  0., -0., -0.,  0.],\n",
       "         [-0., -0.,  0.,  0., -0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0., -0., -0.,  0.],\n",
       "         [-0., -0.,  0.,  0., -0.]],\n",
       "\n",
       "        [[ 0.,  0., -0., -0.,  0.],\n",
       "         [ 0.,  0., -0., -0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(computed - X).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "This notebook provides two contributions:\n",
    "\n",
    "- A simple alternating least squares implementation of a simple array decomposition\n",
    "- A demonstration of the ability of Dask.array to easily parallelize existing NumPy algorithms\n",
    "\n",
    "Future work on this notebook could include:\n",
    "\n",
    "- Improving the algorithm with better initialization conditions or stopping criteria\n",
    "- Performing scaling and benchmarking experiments to study how the parallel/distributed version performs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
