{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples - DataFrame - Dataframes on a cluster\n",
    "http://matthewrocklin.com/blog/work/2017/01/12/dask-dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Dask Dataframe extends the popular Pandas library to operate on big data-sets on a distributed cluster. We show its capabilities by running through common dataframe operations on a common dataset. We break up these computations into the following sections:\n",
    "\n",
    "1. Introduction: Pandas is intuitive and fast, but needs Dask to scale\n",
    "2. Read CSV and Basic operations\n",
    " 1. Read CSV\n",
    " 2. Basic Aggregations and Groupbys\n",
    " 3. Joins and Correlations\n",
    "3. Shuffles and Time Series\n",
    "4. Parquet I/O\n",
    "5. Final thoughts\n",
    "6. What we could have done better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accompanying Plots\n",
    "Throughout this post we accompany computational examples with profiles of exactly what task ran where on our cluster and when. These profiles are interactive Bokeh plots that include every task that every worker in our cluster runs over time. For example the following computation read_csv computation produces the following profile:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask.dataframe breaks up reading this data into many small tasks of different types. For example reading bytes and parsing those bytes into pandas dataframes. Each rectangle corresponds to one task. The y-axis enumerates each of the worker processes. We have 64 processes spread over 8 machines so there are 64 rows. You can hover over any rectangle to get more information about that task. You can also use the tools in the upper right to zoom around and focus on different regions in the computation. In this computation we can see that workers interleave reading bytes from S3 (light green) and parsing bytes to dataframes (dark green). The entire computation took about a minute and most of the workers were busy the entire time (little white space). Inter-worker communication is always depicted in red (which is absent in this relatively straightforward computation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Pandas provides an intuitive, powerful, and fast data analysis experience on tabular data. However, because Pandas uses only one thread of execution and requires all data to be in memory at once, it doesn’t scale well to datasets much beyond the gigabyte scale. That component is missing. Generally people move to Spark DataFrames on HDFS or a proper relational database to resolve this scaling issue. Dask is a Python library for parallel and distributed computing that aims to fill this need for parallelism among the PyData projects (NumPy, Pandas, Scikit-Learn, etc.). Dask dataframes combine Dask and Pandas to deliver a faithful “big data” version of Pandas operating in parallel over a cluster.\n",
    "\n",
    "I’ve written about this topic before. This blogpost is newer and will focus on performance and newer features like fast shuffles and the Parquet format.\n",
    "\n",
    "## CSV Data and Basic Operations\n",
    "I have an eight node cluster on EC2 of m4.2xlarges (eight cores, 30GB RAM each). Dask is running on each node with one process per core.\n",
    "\n",
    "We have the 2015 Yellow Cab NYC Taxi data as 12 CSV files on S3. We look at that data briefly with s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dask-data/nyc-taxi/2015/yellow_tripdata_2015-01.csv',\n",
       " 'dask-data/nyc-taxi/2015/yellow_tripdata_2015-02.csv',\n",
       " 'dask-data/nyc-taxi/2015/yellow_tripdata_2015-03.csv',\n",
       " 'dask-data/nyc-taxi/2015/yellow_tripdata_2015-04.csv',\n",
       " 'dask-data/nyc-taxi/2015/yellow_tripdata_2015-05.csv',\n",
       " 'dask-data/nyc-taxi/2015/yellow_tripdata_2015-06.csv',\n",
       " 'dask-data/nyc-taxi/2015/yellow_tripdata_2015-07.csv',\n",
       " 'dask-data/nyc-taxi/2015/yellow_tripdata_2015-08.csv',\n",
       " 'dask-data/nyc-taxi/2015/yellow_tripdata_2015-09.csv',\n",
       " 'dask-data/nyc-taxi/2015/yellow_tripdata_2015-10.csv',\n",
       " 'dask-data/nyc-taxi/2015/yellow_tripdata_2015-11.csv',\n",
       " 'dask-data/nyc-taxi/2015/yellow_tripdata_2015-12.csv',\n",
       " 'dask-data/nyc-taxi/2015/parquet.gz',\n",
       " 'dask-data/nyc-taxi/2015/parquet',\n",
       " 'dask-data/nyc-taxi/2015/yellow_tripdata_2015-01.parq']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from s3fs import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem(anon=True)\n",
    "s3.ls('dask-data/nyc-taxi/2015/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is too large to fit into Pandas on a single computer. However, it can fit in memory if we break it up into many small pieces and load these pieces onto different computers across a cluster.\n",
    "\n",
    "We connect a client to our Dask cluster, composed of one centralized dask-scheduler process and several dask-worker processes running on each of the machines in our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:45503\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>8.28 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:45503' processes=2 cores=2>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we load our CSV data using dask.dataframe which looks and feels just like Pandas, even though it’s actually coordinating hundreds of small Pandas dataframes. This takes about a minute to load and parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>RateCodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=365</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int64</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from-delayed, 1095 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count trip_distance pickup_longitude pickup_latitude RateCodeID store_and_fwd_flag dropoff_longitude dropoff_latitude payment_type fare_amount    extra  mta_tax tip_amount tolls_amount improvement_surcharge total_amount\n",
       "npartitions=365                                                                                                                                                                                                                                                                                          \n",
       "                   int64       datetime64[ns]        datetime64[ns]           int64       float64          float64         float64      int64             object           float64          float64        int64     float64  float64  float64    float64      float64               float64      float64\n",
       "                     ...                  ...                   ...             ...           ...              ...             ...        ...                ...               ...              ...          ...         ...      ...      ...        ...          ...                   ...          ...\n",
       "...                  ...                  ...                   ...             ...           ...              ...             ...        ...                ...               ...              ...          ...         ...      ...      ...        ...          ...                   ...          ...\n",
       "                     ...                  ...                   ...             ...           ...              ...             ...        ...                ...               ...              ...          ...         ...      ...      ...        ...          ...                   ...          ...\n",
       "                     ...                  ...                   ...             ...           ...              ...             ...        ...                ...               ...              ...          ...         ...      ...      ...        ...          ...                   ...          ...\n",
       "Dask Name: from-delayed, 1095 tasks"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv('s3://dask-data/nyc-taxi/2015/*.csv',\n",
    "                 parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "                 storage_options={'anon': True})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.dataframe.core.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = client.persist(df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cuts up our 12 CSV files on S3 into a few hundred blocks of bytes, each 64MB large. On each of these 64MB blocks we then call pandas.read_csv to create a few hundred Pandas dataframes across our cluster, one for each block of bytes. Our single Dask Dataframe object, df, coordinates all of those Pandas dataframes. Because we’re just using Pandas calls it’s very easy for Dask dataframes to use all of the tricks from Pandas. For example we can use most of the keyword arguments from pd.read_csv in dd.read_csv without having to relearn anything.\n",
    "\n",
    "This data is about 20GB on disk or 60GB in RAM. It’s not huge, but is also larger than we’d like to manage on a laptop, especially if we value interactivity. The interactive image above is a trace over time of what each of our 64 cores was doing at any given moment. By hovering your mouse over the rectangles you can see that cores switched between downloading byte ranges from S3 and parsing those bytes with pandas.read_csv.\n",
    "\n",
    "Our dataset includes every cab ride in the city of New York in the year of 2015, including when and where it started and stopped, a breakdown of the fare, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>RateCodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27191</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-12-31 23:59:56</td>\n",
       "      <td>2016-01-01 00:08:18</td>\n",
       "      <td>5</td>\n",
       "      <td>1.20</td>\n",
       "      <td>-73.993813</td>\n",
       "      <td>40.720871</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.986214</td>\n",
       "      <td>40.722469</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27192</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-12-31 23:59:58</td>\n",
       "      <td>2016-01-01 00:05:19</td>\n",
       "      <td>2</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-73.965271</td>\n",
       "      <td>40.760281</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.939514</td>\n",
       "      <td>40.752388</td>\n",
       "      <td>2</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27193</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-12-31 23:59:59</td>\n",
       "      <td>2016-01-01 00:12:55</td>\n",
       "      <td>2</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-73.987297</td>\n",
       "      <td>40.739079</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.988670</td>\n",
       "      <td>40.693298</td>\n",
       "      <td>2</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>14.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27194</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-12-31 23:59:59</td>\n",
       "      <td>2016-01-01 00:10:26</td>\n",
       "      <td>1</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-73.997559</td>\n",
       "      <td>40.725693</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.017120</td>\n",
       "      <td>40.705322</td>\n",
       "      <td>2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27195</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-12-31 23:59:59</td>\n",
       "      <td>2016-01-01 00:21:30</td>\n",
       "      <td>1</td>\n",
       "      <td>1.06</td>\n",
       "      <td>-73.984398</td>\n",
       "      <td>40.767258</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.990982</td>\n",
       "      <td>40.760571</td>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "27191         2  2015-12-31 23:59:56   2016-01-01 00:08:18                5   \n",
       "27192         1  2015-12-31 23:59:58   2016-01-01 00:05:19                2   \n",
       "27193         1  2015-12-31 23:59:59   2016-01-01 00:12:55                2   \n",
       "27194         2  2015-12-31 23:59:59   2016-01-01 00:10:26                1   \n",
       "27195         2  2015-12-31 23:59:59   2016-01-01 00:21:30                1   \n",
       "\n",
       "       trip_distance  pickup_longitude  pickup_latitude  RateCodeID  \\\n",
       "27191           1.20        -73.993813        40.720871           1   \n",
       "27192           2.00        -73.965271        40.760281           1   \n",
       "27193           3.80        -73.987297        40.739079           1   \n",
       "27194           1.96        -73.997559        40.725693           1   \n",
       "27195           1.06        -73.984398        40.767258           1   \n",
       "\n",
       "      store_and_fwd_flag  dropoff_longitude  dropoff_latitude  payment_type  \\\n",
       "27191                  N         -73.986214         40.722469             1   \n",
       "27192                  N         -73.939514         40.752388             2   \n",
       "27193                  N         -73.988670         40.693298             2   \n",
       "27194                  N         -74.017120         40.705322             2   \n",
       "27195                  N         -73.990982         40.760571             1   \n",
       "\n",
       "       fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "27191          7.5    0.5      0.5        1.76           0.0   \n",
       "27192          7.5    0.5      0.5        0.00           0.0   \n",
       "27193         13.5    0.5      0.5        0.00           0.0   \n",
       "27194          8.5    0.5      0.5        0.00           0.0   \n",
       "27195         13.5    0.5      0.5        2.96           0.0   \n",
       "\n",
       "       improvement_surcharge  total_amount  \n",
       "27191                    0.3         10.56  \n",
       "27192                    0.3          8.80  \n",
       "27193                    0.3         14.80  \n",
       "27194                    0.3          9.80  \n",
       "27195                    0.3         17.76  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Aggregations and Groupbys\n",
    "As a quick exercise, we compute the length of the dataframe. When we call len(df) Dask.dataframe translates this into many len calls on each of the constituent Pandas dataframes, followed by communication of the intermediate results to one node, followed by a sum of all of the intermediate lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)\n",
    "\n",
    "# 146112989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes around 400-500ms. You can see that a few hundred length computations happened quickly on the left, followed by some delay, then a bit of data transfer (the red bar in the plot), and a final summation call.\n",
    "\n",
    "More complex operations like simple groupbys look similar, although sometimes with more communications. Throughout this post we’re going to do more and more complex computations and our profiles will similarly become more and more rich with information. Here we compute the average trip distance, grouped by number of passengers. We find that single and double person rides go far longer distances on average. We acheive this one big-data-groupby by performing many small Pandas groupbys and then cleverly combining their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df.passenger_count).trip_distance.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a more complex operation we see how well New Yorkers tip by hour of day and by day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[(df.tip_amount > 0) & (df.fare_amount > 0)]    # filter out bad rows\n",
    "df2['tip_fraction'] = df2.tip_amount / df2.fare_amount  # make new column\n",
    "\n",
    "dayofweek = (df2.groupby(df2.tpep_pickup_datetime.dt.dayofweek)\n",
    "                .tip_fraction\n",
    "                .mean())\n",
    "hour      = (df2.groupby(df2.tpep_pickup_datetime.dt.hour)\n",
    "                .tip_fraction\n",
    "                .mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress(dayofweek, hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_notebook, show\n",
    "output_notebook()\n",
    "\n",
    "fig = figure(title='Tip Fraction',\n",
    "             x_axis_label='Hour of day',\n",
    "             y_axis_label='Tip Fraction',\n",
    "             height=300)\n",
    "fig.line(x=hour.index.compute(), y=hour.compute(), line_width=3)\n",
    "fig.y_range.start = 0\n",
    "\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins and Correlations\n",
    "To show off more basic functionality we’ll join this Dask dataframe against a smaller Pandas dataframe that includes names of some of the more cryptic columns. Then we’ll correlate two derived columns to determine if there is a relationship between paying Cash and the recorded tip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments = pd.Series({1: 'Credit Card',\n",
    "                          2: 'Cash',\n",
    "                          3: 'No Charge',\n",
    "                          4: 'Dispute',\n",
    "                          5: 'Unknown',\n",
    "                          6: 'Voided trip'})\n",
    "\n",
    "df2 = df.merge(payments, left_on='payment_type', right_index=True)\n",
    "df2.groupby(df2.payment_name).tip_amount.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_tip = df2.tip_amount == 0\n",
    "cash     = df2.payment_name == 'Cash'\n",
    "\n",
    "dd.concat([zero_tip, cash], axis=1).corr().compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
