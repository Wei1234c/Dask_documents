{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples - Bag\n",
    "https://dask.pydata.org/en/latest/examples-tutorials.html#bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "data_path = '../dask-tutorial/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read JSON records from disk\n",
    "We commonly use dask.bag to process unstructured or semi-structured data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import json\n",
    "\n",
    "js = db.read_text(data_path + '/json/2015-*.json').map(json.loads) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'location': {'city': 'LA', 'state': 'CA'}, 'name': 'Alice'},\n",
       " {'location': {'city': 'NYC', 'state': 'NY'}, 'name': 'Bob'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alice': 2, 'Bob': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = js.pluck('name').frequencies()  # just another Bag\n",
    "dict(result)                             # Evaluate Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count\n",
    "In this example, we’ll use dask to count the number of words in text files (Enron email dataset, 6.4 GB) both locally and on a cluster (along with the distributed and hdfs3 libraries).\n",
    "\n",
    "### Local computation\n",
    "Download the first text file (76 MB) in the dataset to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.bag<bag-fro..., npartitions=2>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "b = db.read_text('merged.txt', blocksize=1000)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"('Date: Tue, 26 Sep 2000 09:26:00 -0700 (PDT)\\\\r\\\\n',\\n\",\n",
       " \" 'From: Phillip K Allen\\\\r\\\\n',\\n\",\n",
       " \" 'To: pallen70@hotmail.com\\\\r\\\\n',\\n\",\n",
       " \" 'Subject: Investment Structure\\\\r\\\\n',\\n\",\n",
       " \" 'X-SDOC: 948896\\\\r\\\\n',\\n\",\n",
       " \" 'X-ZLID: zl-edrm-enron-v2-allen-p-1713.eml\\\\r\\\\n',\\n\",\n",
       " \" '\\\\r\\\\n',\\n\",\n",
       " \" '---------------------- Forwarded by Phillip K Allen/HOU/ECT on 09/26/2000 \\\\r\\\\n',\\n\",\n",
       " \" '04:26 PM ---------------------------\\\\r\\\\n',\\n\",\n",
       " \" '\\\\r\\\\n')\\n\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a word count expression using the bag methods to split the lines into words, concatenate the nested lists of words into a single list, count the frequencies of each word, then list the top 10 words by their count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.bag<topk-ag..., npartitions=1>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount = b.str.split().flatten().frequencies().topk(10, lambda x: x[1])\n",
    "wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the combined operations in the previous expression are lazy. We can trigger the word count computation using .compute():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Phillip', 6),\n",
       " ('K', 6),\n",
       " (\"('Date:\", 3),\n",
       " ('Tue,', 3),\n",
       " ('26', 3),\n",
       " ('Sep', 3),\n",
       " ('2000', 3),\n",
       " ('09:26:00', 3),\n",
       " ('-0700', 3),\n",
       " (\"(PDT)\\\\r\\\\n',\", 3)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster computation with HDFS\n",
    "Next, we’ll use dask along with the distributed and hdfs3 libraries to count the number of words in all of the text files stored in a Hadoop Distributed File System (HDFS).\n",
    "\n",
    "Copy the text data from Amazon S3 into HDFS on the cluster:\n",
    "```shell\n",
    "$ hadoop distcp s3n://AWS_SECRET_ID:AWS_SECRET_KEY@blaze-data/enron-email hdfs:///tmp/enron\n",
    "```\n",
    "where AWS_SECRET_ID and AWS_SECRET_KEY are valid AWS credentials.\n",
    "\n",
    "We can now start a distributed scheduler and workers on the cluster, replacing SCHEDULER_IP and SCHEDULER_PORT with the IP address and port of the distributed scheduler:\n",
    "```sh\n",
    "$ dask-scheduler  # On the head node\n",
    "$ dask-worker SCHEDULER_IP:SCHEDULER_PORT --nprocs 4 --nthreads 1  # On the compute nodes\n",
    "```\n",
    "Because our computations use pure Python rather than numeric libraries (e.g., NumPy, pandas), we started the workers with multiple processes rather than with multiple threads. This helps us avoid issues with the Python Global Interpreter Lock (GIL) and increases efficiency.\n",
    "\n",
    "In Python, import the hdfs3 and the distributed methods used in this example:\n",
    "```python\n",
    "from dask.distributed import Client, progress\n",
    "```\n",
    "Initialize a connection to the distributed executor:\n",
    "```python\n",
    "client = Client('SCHEDULER_IP:SCHEDULER_PORT')\n",
    "```\n",
    "Create a bag from the text files stored in HDFS. This expression will not read data from HDFS until the computation is triggered:\n",
    "```python\n",
    "import dask.bag as db\n",
    "b = db.read_text('hdfs:///tmp/enron/*/*')\n",
    "```\n",
    "We can write a word count expression using the same bag methods as the local dask example:\n",
    "```python\n",
    "wordcount = b.str.split().flatten().frequencies().topk(10, lambda x: x[1])\n",
    "```\n",
    "We are ready to count the number of words in all of the text files using distributed workers. We can map the wordcount expression to a future that triggers the computation on the cluster.\n",
    "```python\n",
    "future = clinet.compute(wordcount)\n",
    "```\n",
    "Note that the compute operation is **non-blocking**, and you can continue to work in the Python shell/notebook while the computations are running.\n",
    "\n",
    "We can check the status of the future while all of the text files are being processed:\n",
    "```python\n",
    "print(future)\n",
    "<Future: status: pending, key: finalize-0f2f51e2350a886223f11e5a1a7bc948>\n",
    "\n",
    "progress(future)\n",
    "[########################################] | 100% Completed |  8min  15.2s\n",
    "```\n",
    "This computation required about 8 minutes to run on a cluster with three worker machines, each with 4 cores and 16 GB RAM. For comparison, running the same computation locally with dask required about 20 minutes on a single machine with the same specs.\n",
    "\n",
    "When the future finishes reading in all of the text files and counting words, the results will exist on each worker. To sum the word counts for all of the text files, we need to gather the results from the dask.distributed workers:\n",
    "```python\n",
    "results = client.gather(future)\n",
    "```\n",
    "Finally, we print the top 10 words from all of the text files:\n",
    "```python\n",
    "print(results)\n",
    "[('0', 67218227),\n",
    " ('the', 19588747),\n",
    " ('-', 14126955),\n",
    " ('to', 11893912),\n",
    " ('N/A', 11814994),\n",
    " ('of', 11725144),\n",
    " ('and', 10254267),\n",
    " ('in', 6685245),\n",
    " ('a', 5470711),\n",
    " ('or', 5227787)]\n",
    " ```\n",
    "The complete Python script for this example is shown below:\n",
    "```python\n",
    "# word-count.py\n",
    "\n",
    "# Local computation\n",
    "\n",
    "import dask.bag as db\n",
    "b = db.read_text('merged.txt')\n",
    "b.take(10)\n",
    "wordcount = b.str.split().flatten().frequencies().topk(10, lambda x: x[1])\n",
    "wordcount.compute()\n",
    "\n",
    "\n",
    "# Cluster computation with HDFS\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "client = Client('SCHEDULER_IP:SCHEDULER_PORT')\n",
    "\n",
    "b = db.read_text('hdfs:///tmp/enron/*/*')\n",
    "wordcount = b.str.split().flatten().frequencies().topk(10, lambda x: x[1])\n",
    "\n",
    "future = client.compute(wordcount)\n",
    "print(future)\n",
    "progress(future)\n",
    "\n",
    "results = client.gather(future)\n",
    "print(results)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
