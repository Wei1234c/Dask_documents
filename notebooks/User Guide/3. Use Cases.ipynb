{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cases\n",
    "https://dask.pydata.org/en/latest/use-cases.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2015-03-27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2015-03-28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2015-03-29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2015-03-30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   x\n",
       "85 2015-03-27  27\n",
       "86 2015-03-28  28\n",
       "87 2015-03-29  29\n",
       "88 2015-03-30  30\n",
       "89 2015-03-31  31"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DataFrame({'date': pd.date_range(start = '2015-1-1', end = '2015-3-31')})\n",
    "df['x'] = df.date.dt.day\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '2015.csv'\n",
    "# df.to_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Dask use cases can be roughly divided in the following two categories:\n",
    "\n",
    "1. Large NumPy/Pandas/Lists with dask.array, dask.dataframe, dask.bag to analyze large datasets with familiar techniques. This is similar to Databases, Spark, or big array libraries.\n",
    "2. Custom task scheduling. You submit a graph of functions that depend on each other for custom workloads. This is similar to Luigi, Airflow, Celery, or Makefiles.  \n",
    "\n",
    "Most people today approach Dask assuming it is a framework like Spark, designed for the first use case around large collections of uniformly shaped data. However, many of the more productive and novel use cases fall into the second category, using Dask to parallelize custom workflows.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dask compute environments can be divided into the following two categories:\n",
    "\n",
    "1. Single machine parallelism with threads or processes: The Dask single-machine scheduler leverages the full CPU power of a laptop or a large workstation and changes the space limitation from “fits in memory” to “fits on disk”. This scheduler is simple to use and doesn’t have the computational or conceptual overhead of most “big data” systems.\n",
    "2. Distributed cluster parallelism on multiple nodes: The Dask distributed scheduler coordinates the actions of multiple machines on a cluster. It scales anywhere from a single machine to a thousand machines, but not significantly beyond.\n",
    "\n",
    "\n",
    "The single machine scheduler is useful to more individuals (more people have personal laptops than have access to clusters) and probably accounts for 80+% of the use of Dask today. The distributed machine scheduler is useful to larger organizations like universities, research labs, or private companies.\n",
    "\n",
    "Below we give specific examples of how people use Dask. We start with large NumPy/Pandas/List examples because they’re somewhat more familiar to people looking at “big data” frameworks. We then follow with custom scheduling examples, which tend to be applicable more often, and are arguably a bit more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection Examples\n",
    "Dask contains large parallel collections for n-dimensional arrays (similar to NumPy), dataframes (similar to Pandas), and lists (similar to PyToolz or PySpark).\n",
    "\n",
    "### On disk arrays\n",
    " They use dask.array to treat this stack of HDF5 or NetCDF files as a single NumPy array (or a collection of NumPy arrays with the XArray project). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Softwares\\Python\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py, os\n",
    "import numpy as np\n",
    "\n",
    "# path = 'data'\n",
    "path = '../dask-tutorial/data'\n",
    "filename = 'myfile.hdf5'\n",
    "file = os.path.join(path, filename)\n",
    "\n",
    "total_size = 1000000\n",
    "chunks = 1000\n",
    "\n",
    "if not os.path.exists(file):\n",
    "    with h5py.File(file) as f:\n",
    "        dset = f.create_dataset('/x', \n",
    "                                shape = (total_size,), \n",
    "                                dtype = np.float32,\n",
    "                                chunks=(chunks,),\n",
    "                                compression='gzip',\n",
    "                                compression_opts=9) \n",
    "        \n",
    "        for i in range(0, total_size, chunks):\n",
    "            dset[i: i + chunks] = np.random.exponential(size=chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:54608\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>8.48 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:54608' processes=4 cores=4>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.array as da\n",
    "from distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [-0.64674985 -0.22668874 -0.27774346  2.559743    0.2979015  -0.5956613\n",
      "  0.79906476 -0.25978506 -0.6400137  -0.50122947 -0.5501237  -0.45786148\n",
      " -0.81786805 -0.6803148   1.8183072   0.0706774  -0.34766126 -0.2607717\n",
      " -0.9230949  -0.40851784 -0.2787838  -0.74477565 -0.8358296  -0.7863423\n",
      "  0.9568     -0.5427391   1.3723187  -0.08791035  2.8399603  -0.7163671\n",
      "  1.5204368  -0.7691104  -0.52979404 -0.50032794 -0.5012726  -0.6958561\n",
      "  0.10943246 -0.29757774  2.0415967  -0.6752656  -0.5851799  -0.6956012\n",
      " -0.7376877   0.04875898  0.4183973   0.53458846 -0.29217488 -0.68081343\n",
      "  0.38699257 -0.42110473  0.73694146 -0.56270605 -0.60957325  0.28478825\n",
      "  0.08592284  0.05933285 -0.88665557 -0.88949794 -0.957826   -0.7015122\n",
      "  1.2569599  -0.7414051   0.30147684  1.1287274   2.2218735  -0.30781156\n",
      " -0.50330496  0.51573336  1.5444198   2.7049658   0.9826366   0.74960566\n",
      " -0.4028632   0.36163366 -0.8244845  -0.1751982  -0.10236657 -0.3164003\n",
      " -0.76260257  0.37822425 -0.03461301  0.96427107  1.3644836  -0.30679017\n",
      "  0.8786169  -0.90976965  0.7559854  -0.4241243   0.5744313   0.12286592\n",
      " -0.7376217  -0.6392548  -0.43501604  0.74792683  0.8836386  -0.85963583\n",
      " -0.41198307  0.1535933  -0.9707407  -0.83549273]\n",
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "h5file = h5py.File(file, mode = 'r')\n",
    "dataset = h5file['/x']\n",
    "\n",
    "# x = dataset[...]\n",
    "x = da.from_array(dataset, chunks = dataset.chunks)\n",
    "y = x[::10000] - x.mean(axis=0)\n",
    "\n",
    "%time print('result:', y.compute())\n",
    "\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory of CSV or tabular HDF files\n",
    "They use dask.dataframe to logically wrap all of these different files into one logical dataframe that is built on demand to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-02-28 05:00:00</td>\n",
       "      <td>1.756584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-02-28 06:00:00</td>\n",
       "      <td>0.581455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-02-28 07:00:00</td>\n",
       "      <td>-0.172263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-02-28 08:00:00</td>\n",
       "      <td>0.501906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-02-28 09:00:00</td>\n",
       "      <td>-0.016529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp     value\n",
       "5 2015-02-28 05:00:00  1.756584\n",
       "6 2015-02-28 06:00:00  0.581455\n",
       "7 2015-02-28 07:00:00 -0.172263\n",
       "8 2015-02-28 08:00:00  0.501906\n",
       "9 2015-02-28 09:00:00 -0.016529"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../dask-tutorial/data/csv/'\n",
    "dates = pd.date_range(start = '2015-1-1', end = '2015-2-28') \n",
    "\n",
    "for date in dates:\n",
    "    filename = str(date.date())\n",
    "    file = os.path.join(path, filename)\n",
    "    \n",
    "    timedeltas = pd.timedelta_range(start = 0, periods = 10, freq = '1H')\n",
    "    df = DataFrame({'timestamp': date + timedeltas})\n",
    "    df['value'] = np.random.randn(timedeltas.shape[0])\n",
    "    \n",
    "    df.to_csv(file + '.csv') \n",
    "    \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp\n",
       "0   -0.191419\n",
       "1    0.142274\n",
       "2   -0.024639\n",
       "3   -0.147495\n",
       "4    0.141604\n",
       "5    0.138348\n",
       "6   -0.148399\n",
       "7    0.087410\n",
       "8   -0.094926\n",
       "9    0.009555\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(path + '2015-*-*.csv',\n",
    "                 parse_dates=['timestamp'])\n",
    "\n",
    "value_mean = df.groupby(df.timestamp.dt.hour).value.mean()\n",
    "value_mean.compute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=1\n",
       "    float64\n",
       "        ...\n",
       "Name: value, dtype: float64\n",
       "Dask Name: truediv, 432 tasks"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.dataframe.core.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(value_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "value_mean.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory of CSV files on HDFS\n",
    "The same analyst as above uses dask.dataframe with the dask.distributed scheduler to analyze terabytes of data on their institution’s Hadoop cluster straight from Python. This uses either the hdfs3 or pyarrow Python libraries for HDFS management\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client('cluster-address:8786')\n",
    "\n",
    "import dask.dataframe as dd\n",
    "df = dd.read_csv('hdfs://data/2016-*.*.csv', parse_dates=['timestamp'])\n",
    "df.groupby(df.timestamp.dt.hour).value.mean().compute()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories of custom format files\n",
    "https://gist.github.com/mrocklin/e7b7b3a65f2835cda813096332ec73ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON data\n",
    "Data Engineers with click stream data from a website or mechanical engineers with telemetry data from mechanical instruments have large volumes of data in JSON or some other semi-structured format. They use dask.bag to manipulate many Python objects in parallel either on their personal machine, where they stream the data through memory or across a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dask-tutorial/data/json/'\n",
    "dates = pd.date_range(start = '2015-1-1', end = '2015-2-28') \n",
    "\n",
    "for date in dates:\n",
    "    filename = str(date.date())\n",
    "    file = os.path.join(path, filename)\n",
    "    b = db.from_sequence([{'name': 'Alice', 'id': 123}] * 20)\n",
    "    b = db.from_sequence(np.random.randn(20))\n",
    "    \n",
    "    b.to_textfiles(file + '-*.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = db.read_text('data/2015-*-*.json').map(json.loads)\n",
    "records.filter(lambda d: d['name'] == 'Alice').pluck('id').frequencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Examples\n",
    "The large collections (array, dataframe, bag) are wonderful when they fit the application, for example if you want to perform a groupby on a directory of CSV data. However several parallel computing applications don’t fit neatly into one of these higher level abstractions. Fortunately, Dask provides a wide variety of ways to parallelize more custom applications. These use the same machinery as the arrays and dataframes, but allow the user to develop custom algorithms specific to their problem.\n",
    "\n",
    "### Embarrassingly parallel computation\n",
    "A programmer has a function that they want to run many times on different inputs. Their function and inputs might use arrays or dataframes internally, but conceptually their problem isn’t a single large array or dataframe.\n",
    "\n",
    "They want to run these functions in parallel on their laptop while they prototype but they also intend to eventually use an in-house cluster. They wrap their function in dask.delayed and let the appropriate dask scheduler parallelize and load balance the work.\n",
    "```python\n",
    "def process(data):\n",
    "   ...\n",
    "   return ...\n",
    "```\n",
    "#### Normal Sequential Processing:\n",
    "```python\n",
    "results = [process(x) for x in inputs]\n",
    "```\n",
    "#### Build Dask Computation:\n",
    "```python\n",
    "from dask import compute, delayed\n",
    "values = [delayed(process)(x) for x in inputs]\n",
    "```\n",
    "#### Multiple Threads:\n",
    "```python\n",
    "import dask.threaded\n",
    "results = compute(*values, get=dask.threaded.get)\n",
    "```\n",
    "#### Multiple Processes:\n",
    "```python\n",
    "import dask.multiprocessing\n",
    "results = compute(*values, get=dask.multiprocessing.get)\n",
    "```\n",
    "#### Distributed Cluster:\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client(\"cluster-address:8786\")\n",
    "results = compute(*values, get=client.get)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex dependencies\n",
    "A financial analyst has many models that depend on each other in a complex web of computations.\n",
    "```python\n",
    "data = [load(fn) for fn in filenames]\n",
    "reference = load_from_database(query)\n",
    "\n",
    "A = [model_a(x, reference) for x in data]\n",
    "B = [model_b(x, reference) for x in data]\n",
    "\n",
    "roll_A = [roll(A[i], A[i + 1]) for i in range(len(A) - 1)]\n",
    "roll_B = [roll(B[i], B[i + 1]) for i in range(len(B) - 1)]\n",
    "compare = [compare_ab(a, b) for a, b in zip(A, B)]\n",
    "\n",
    "results = summarize(compare, roll_A, roll_B)\n",
    "```\n",
    "These models are time consuming and need to be run on a variety of inputs and situations. The analyst has his code now as a collection of Python functions and is trying to figure out how to parallelize such a codebase. They use dask.delayed to wrap their function calls and capture the implicit parallelism.\n",
    "```python\n",
    "from dask import compute, delayed\n",
    "\n",
    "data = [delayed(load)(fn) for fn in filenames]\n",
    "reference = delayed(load_from_database)(query)\n",
    "\n",
    "A = [delayed(model_a)(x, reference) for x in data]\n",
    "B = [delayed(model_b)(x, reference) for x in data]\n",
    "\n",
    "roll_A = [delayed(roll)(A[i], A[i + 1]) for i in range(len(A) - 1)]\n",
    "roll_B = [delayed(roll)(B[i], B[i + 1]) for i in range(len(B) - 1)]\n",
    "compare = [delayed(compare_ab)(a, b) for a, b in zip(A, B)]\n",
    "\n",
    "lazy_results = delayed(summarize)(compare, roll_A, roll_B)\n",
    "```\n",
    "They then depend on the dask schedulers to run this complex web of computations in parallel.\n",
    "```python\n",
    "results = compute(lazy_results)\n",
    "```\n",
    "They appreciate how easy it was to transition from the experimental code to a scalable parallel version. This code is also easy enough for their teammates to understand easily and extend in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm developer\n",
    "A graduate student in machine learning is prototyping novel parallel algorithms. They don’t have access to an institutional cluster, so instead they use dask-ec2 to easily provision clusters of varying sizes.\n",
    "\n",
    "**Their algorithm is written the same in all cases**, drastically reducing the cognitive load, and letting the readers of their work experiment with their system on their own machines, aiding reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn or Joblib User\n",
    "A data scientist wants to scale their machine learning pipeline to run on their cluster to accelerate parameter searches. They already use the sklearn **njobs=** parameter to accelerate their computation on their local computer with Joblib. Now they wrap their sklearn code with a context manager to parallelize the exact same code across a cluster (also available with IPyParallel)\n",
    "```python\n",
    "import distributed.joblib\n",
    "\n",
    "with joblib.parallel_backend('distributed',\n",
    "                             scheduler_host=('192.168.1.100', 8786)):\n",
    "    result = GridSearchCV( ... )  # normal sklearn code\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Academic Cluster Administrator\n",
    "A system administrator for a university compute cluster wants to enable many researchers to use the available cluster resources, which are currently lying idle. The research faculty and graduate students lack experience with job schedulers and MPI, but are comfortable interacting with Python code through a Jupyter notebook.\n",
    "\n",
    "Teaching the faculty and graduate students to parallelize software has proven time consuming. Instead the administrator sets up dask.distributed on a sandbox allocation of the cluster and broadly publishes the address of the scheduler, pointing researchers to the dask.distributed quickstart. Utilization of the cluster climbs steadily over the next week as researchers are more easily able to parallelize their computations without having to learn foreign interfaces. The administrator is happy because resources are being used without significant hand-holding.\n",
    "\n",
    "As utilization increases the administrator has a new problem; the shared dask.distributed cluster is being overused. The administrator tracks use through Dask diagnostics to identify which users are taking most of the resources. They contact these users and teach them how to launch their own dask.distributed clusters using the traditional job scheduler on their cluster, making space for more new users in the sandbox allocation.\n",
    "\n",
    "### Financial Modeling Team\n",
    "Similar to the case above, a team of modelers working at a financial institution run a complex network of computational models on top of each other. They started using dask.delayed individually, as suggested above, but realized that they often perform highly overlapping computations, such as always reading the same data.\n",
    "\n",
    "Now they decide to use the same Dask cluster collaboratively to save on these costs. Because Dask intelligently hashes computations in a way similar to how Git works, they find that when two people submit similar computations the overlapping part of the computation runs only once.\n",
    "\n",
    "Ever since working collaboratively on the same cluster they find that their frequently running jobs run much faster, because most of the work is already done by previous users. When they share scripts with colleagues they find that those repeated scripts complete immediately rather than taking several hours.\n",
    "\n",
    "They are now able to iterate and share data as a team more effectively, decreasing their time to result and increasing their competitive edge.\n",
    "\n",
    "As this becomes more heavily used on the company cluster they decide to set up an auto-scaling system. They use their dynamic job scheduler (perhaps SGE, LSF, Mesos, or Marathon) to run a single dask-scheduler 24/7 and then scale up and down the number of dask-workers running on the cluster based on computational load. This solution ends up being more responsive (and thus more heavily used) than their previous attempts to provide institution-wide access to parallel computing but because it responds to load it still acts as a good citizen in the cluster.\n",
    "\n",
    "### Streaming data engineering\n",
    "A data engineer responsible for watching a data feed needs to scale out a continuous process. They combine dask.distributed with normal Python Queues to produce a rudimentary but effective stream processing system.\n",
    "\n",
    "Because dask.distributed is elastic, they can scale up or scale down their cluster resources in response to demand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
