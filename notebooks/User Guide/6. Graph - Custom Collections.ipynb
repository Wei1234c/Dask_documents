{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph - Custom Collections\n",
    "https://dask.pydata.org/en/latest/custom-collections.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internals of the Core Dask Methods\n",
    "Dask has a few core functions (and corresponding methods) that implement common operations:\n",
    "\n",
    "- `compute`: convert one or more dask collections into their in-memory counterparts\n",
    "- `persist`: convert one or more dask collections into equivalent dask collections with their results already computed and cached in memory.\n",
    "- `optimize`: convert one or more dask collections into equivalent dask collections sharing one large optimized graph.\n",
    "- `visualize`: given one or more dask collections, draw out the graph that would be passed to the scheduler during a call to compute or persist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute\n",
    "In pseudocode this process looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(*collections, **kwargs):\n",
    "    # 1. Graph Merging & Optimization\n",
    "    # -------------------------------\n",
    "    if kwargs.pop('optimize_graph', True):\n",
    "        # If optimization is turned on, group the collections by\n",
    "        # optimization method, and apply each method only once to the merged\n",
    "        # sub-graphs.\n",
    "        optimization_groups = groupby_optimization_methods(collections)\n",
    "        graphs = []\n",
    "        for optimize_method, cols in optimization_groups:\n",
    "            # Merge the graphs and keys for the subset of collections that\n",
    "            # share this optimization method\n",
    "            sub_graph = merge_graphs([x.__dask_graph__() for x in cols])\n",
    "            sub_keys = [x.__dask_keys__() for x in cols]\n",
    "            # kwargs are forwarded to ``__dask_optimize__`` from compute\n",
    "            optimized_graph = optimize_method(sub_graph, sub_keys, **kwargs)\n",
    "            graphs.append(optimized_graph)\n",
    "        graph = merge_graphs(graphs)\n",
    "    else:\n",
    "        graph = merge_graphs([x.__dask_graph__() for x in collections])\n",
    "    # Keys are always the same\n",
    "    keys = [x.__dask_keys__() for x in collections]\n",
    "\n",
    "    # 2. Computation\n",
    "    # --------------\n",
    "    # Determine appropriate get function based on collections, global\n",
    "    # settings, and keyword arguments\n",
    "    get = determine_get_function(collections, **kwargs)\n",
    "    # Pass the merged graph, keys, and kwargs to ``get``\n",
    "    results = get(graph, keys, **kwargs)\n",
    "\n",
    "    # 3. Postcompute\n",
    "    # --------------\n",
    "    output = []\n",
    "    # Iterate over the results and collections\n",
    "    for res, collection in zip(results, collections):\n",
    "        finalize, extra_args = collection.__dask_postcompute__()\n",
    "        out = finalize(res, **extra_args)\n",
    "        output.append(out)\n",
    "\n",
    "    # `dask.compute` always returns tuples\n",
    "    return tuple(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist \n",
    "In pseudocode this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist(*collections, **kwargs):\n",
    "    # 1. Graph Merging & Optimization\n",
    "    # -------------------------------\n",
    "    # **Same as in compute**\n",
    "    graph = ...\n",
    "    keys = ...\n",
    "\n",
    "    # 2. Computation\n",
    "    # --------------\n",
    "    # **Same as in compute**\n",
    "    results = ...\n",
    "\n",
    "    # 3. Postpersist\n",
    "    # --------------\n",
    "    output = []\n",
    "    # Iterate over the results and collections\n",
    "    for res, collection in zip(results, collections):\n",
    "        # res has the same structure as keys\n",
    "        keys = collection.__dask_keys__()\n",
    "        # Get the computed graph for this collection.\n",
    "        # Here flatten converts a nested list into a single list\n",
    "        subgraph = {k: r for (k, r) in zip(flatten(keys), flatten(res))}\n",
    "\n",
    "        # Rebuild the output dask collection with the computed graph\n",
    "        rebuild, extra_args = collection.__dask_postpersist__()\n",
    "        out = rebuild(subgraph, *extra_args)\n",
    "\n",
    "        output.append(out)\n",
    "\n",
    "    # dask.persist always returns tuples\n",
    "    return tuple(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize\n",
    "In pseudocode this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(*collections, **kwargs):\n",
    "    # 1. Graph Merging & Optimization\n",
    "    # -------------------------------\n",
    "    # **Same as in compute**\n",
    "    graph = ...\n",
    "\n",
    "    # 2. Rebuilding\n",
    "    # -------------\n",
    "    # Rebuild each dask collection using the same large optimized graph\n",
    "    output = []\n",
    "    for collection in collections:\n",
    "        rebuild, extra_args = collection.__dask_postpersist__()\n",
    "        out = rebuild(graph, *extra_args)\n",
    "        output.append(out)\n",
    "\n",
    "    # dask.optimize always returns tuples\n",
    "    return tuple(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize n using graphviz and output to the specified file.\n",
    "In pseudocode this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(*collections, **kwargs):\n",
    "    # 1. Graph Merging & Optimization\n",
    "    # -------------------------------\n",
    "    # **Same as in compute**\n",
    "    graph = ...\n",
    "\n",
    "    # 2. Graph Drawing\n",
    "    # ----------------\n",
    "    # Draw the graph with graphviz's `dot` tool and return the result.\n",
    "    return dot_graph(graph, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the Core Dask Methods to Your Class\n",
    "Defining the above interface will allow your object to used by the core dask functions (`dask.compute`, `dask.persist`, `dask.visualize`, etcâ€¦). To add corresponding method versions of these subclass from `dask.base.DaskMethodsMixin`, which adds implementations of `compute`, `persist` and `visualize` based on the interface above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Dask Collection\n",
    "Here we create a dask collection representing a tuple. Every element in the tuple is represented as a task in the graph. Note that this is for illustration purposes only - the same user experience could be done using normal tuples with elements of dask.delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved as dask_tuple.py\n",
    "from dask.base import DaskMethodsMixin\n",
    "from dask.optimization import cull\n",
    "\n",
    "# We subclass from DaskMethodsMixin to add common dask methods to our\n",
    "# class. This is nice but not necessary for creating a dask collection.\n",
    "class Tuple(DaskMethodsMixin):\n",
    "    def __init__(self, dsk, keys):\n",
    "        # The init method takes in a dask graph and a set of keys to use\n",
    "        # as outputs.\n",
    "        self._dsk = dsk\n",
    "        self._keys = keys\n",
    "\n",
    "    def __dask_graph__(self):\n",
    "        return self._dsk\n",
    "\n",
    "    def __dask_keys__(self):\n",
    "        return self._keys\n",
    "\n",
    "    @staticmethod\n",
    "    def __dask_optimize__(dsk, keys, **kwargs):\n",
    "        # We cull unnecessary tasks here. Note that this isn't necessary,\n",
    "        # dask will do this automatically, this just shows one optimization\n",
    "        # you could do.\n",
    "        dsk2, _ = cull(dsk, keys)\n",
    "        return dsk2\n",
    "\n",
    "    # Use the threaded scheduler by default.\n",
    "    __dask_scheduler__ = staticmethod(dask.threaded.get)\n",
    "\n",
    "    def __dask_postcompute__(self):\n",
    "        # We want to return the results as a tuple, so our finalize\n",
    "        # function is `tuple`. There are no extra arguments, so we also\n",
    "        # return an empty tuple.\n",
    "        return tuple, ()\n",
    "\n",
    "    def __dask_postpersist__(self):\n",
    "        # Since our __init__ takes a graph as its first argument, our\n",
    "        # rebuild function can just be the class itself. For extra\n",
    "        # arguments we also return a tuple containing just the keys.\n",
    "        return Tuple, (self._keys,)\n",
    "\n",
    "    def __dask_tokenize__(self):\n",
    "        # For tokenize to work we want to return a value that fully\n",
    "        # represents this object. In this case it's the list of keys\n",
    "        # to be computed.\n",
    "        return tuple(self._keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrating this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# >>> from dask_tuple import Tuple\n",
    ">>> from operator import add, mul\n",
    "\n",
    "# Define a dask graph\n",
    ">>> dsk = {'a': 1,\n",
    "...        'b': 2,\n",
    "...        'c': (add, 'a', 'b'),\n",
    "...        'd': (mul, 'b', 2),\n",
    "...        'e': (add, 'b', 'c')}\n",
    "\n",
    "# The output keys for this graph\n",
    ">>> keys = ['b', 'c', 'd', 'e']\n",
    "\n",
    ">>> x = Tuple(dsk, keys)\n",
    "\n",
    "# Compute turns Tuple into a tuple\n",
    ">>> x.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persist turns Tuple into a Tuple, with each task already computed\n",
    ">>> x2 = x.persist()\n",
    ">>> isinstance(x2, Tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 2, 'c': 3, 'd': 4, 'e': 5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> x2.__dask_graph__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if an object is a dask collection\n",
    "To check if an object is a dask collection, use `dask.base.is_dask_collection`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from dask.base import is_dask_collection\n",
    ">>> from dask import delayed\n",
    "\n",
    ">>> x = delayed(sum)([1, 2, 3])\n",
    ">>> is_dask_collection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_dask_collection(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Deterministic Hashing\n",
    "Dask implements its own deterministic hash function to generate keys based on the value of arguments. This function is available as `dask.base.tokenize`. Many common types already have implementations of `tokenize`, which can be found in `dask/base.py`.\n",
    "\n",
    "When creating your own custom classes you may need to register a `tokenize` implementation. There are two ways to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5988362b6e07087db2bc8e7c1c8cc560'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from dask.base import tokenize, normalize_token\n",
    "\n",
    "# Define a tokenize implementation using a method.\n",
    ">>> class Foo(object):\n",
    "...     def __init__(self, a, b):\n",
    "...         self.a = a\n",
    "...         self.b = b\n",
    "...\n",
    "...     def __dask_tokenize__(self):\n",
    "...         # This tuple fully represents self\n",
    "...         return (Foo, self.a, self.b)\n",
    "\n",
    ">>> x = Foo(1, 2)\n",
    ">>> tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(x) == tokenize(x)  # token is deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5a7e9c3645aa44cf13d021c14452152e'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register an implementation with normalize_token\n",
    ">>> class Bar(object):\n",
    "...     def __init__(self, x, y):\n",
    "...         self.x = x\n",
    "...         self.y = y\n",
    "\n",
    ">>> @normalize_token.register(Bar)\n",
    "... def tokenize_bar(x):\n",
    "...     return (Bar, x.x, x.x)\n",
    "\n",
    ">>> y = Bar(1, 2)\n",
    ">>> tokenize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> tokenize(y) == tokenize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> tokenize(y) == tokenize(x)  # tokens for different objects aren't equal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
